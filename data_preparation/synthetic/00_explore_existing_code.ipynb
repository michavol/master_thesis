{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"### Data\"\"\"\n",
    "\n",
    "import copy\n",
    "\n",
    "def compute_total_polynomial_terms(poly_degree, latent_dim):\n",
    "    \"\"\"\n",
    "    Compute the total number of possible terms for polynomials of degree poly_degree.\n",
    "    \"\"\"\n",
    "    count=0\n",
    "    for degree in range(poly_degree+1):\n",
    "        count+= pow(latent_dim, degree)\n",
    "    return count\n",
    "\n",
    "\n",
    "def compute_kronecker_product(degree, latent):\n",
    "    \"\"\"\n",
    "    Compute the kronecker product of the latent vector with itself for a given degree.\n",
    "    \"\"\"\n",
    "    if degree ==0:\n",
    "        out= torch.tensor([1.])\n",
    "    else:\n",
    "        out=copy.deepcopy(latent)\n",
    "        for idx in range(1, degree):\n",
    "            out= torch.kron(out, latent)\n",
    "    #print(out.shape)\n",
    "    return out\n",
    "\n",
    "def compute_decoder_polynomial(poly_degree,latent):\n",
    "    \"\"\"\n",
    "    Compute all the kronecker products of the latent vector with itself up to degree poly_degree.\n",
    "    \"\"\"\n",
    "    out=[]\n",
    "    for degree in range(poly_degree+1):\n",
    "        kroneck = compute_kronecker_product(degree, latent)\n",
    "        out.append(kroneck)\n",
    "    out= torch.concatenate(out)\n",
    "    out= torch.reshape(out, (1,out.shape[0]))\n",
    "    return out\n",
    "\n",
    "def compute_decoder_polynomial_function(poly_degree):\n",
    "    \"\"\"\n",
    "    Compute the function that computes the polynomial terms of a given degree.\n",
    "    \"\"\"\n",
    "    def poly_decoder(latent):\n",
    "        out=[]\n",
    "        for degree in range(poly_degree+1):\n",
    "    #         print('Computing polynomial term of degree ', degree)\n",
    "            kroneck = compute_kronecker_product(degree, latent)\n",
    "            #print(kroneck.shape)\n",
    "            out.append(kroneck)\n",
    "        out= torch.concatenate(out)\n",
    "        out= torch.reshape(out, (1,out.shape[0]))\n",
    "        return out\n",
    "    return poly_decoder\n",
    "\n",
    "class Poly_dec(nn.Module):\n",
    "    def __init__(self, poly_degree, x_dim, lat_dim):\n",
    "        super().__init__()\n",
    "        self.deg = poly_degree\n",
    "        self.x_dim = x_dim\n",
    "        self.lat_dim = lat_dim\n",
    "\n",
    "        self.poly_size = compute_total_polynomial_terms(poly_degree, lat_dim)\n",
    "        #self.coff_matrix = np.random.multivariate_normal(np.zeros(self.poly_size), np.eye(self.poly_size), size=20).T\n",
    "        self.coff_matrix = self.full_rank_coef_matrix()\n",
    "\n",
    "    def full_rank_coef_matrix(self):\n",
    "        \"\"\"\n",
    "        Generate a full rank coefficient matrix for the polynomial decoder.\n",
    "        \"\"\"\n",
    "        M = torch.distributions.multivariate_normal.MultivariateNormal(torch.zeros(self.poly_size), torch.eye(self.poly_size)).sample((self.x_dim,)).t()\n",
    "        if torch.linalg.matrix_rank(M) == self.poly_size:\n",
    "            return M\n",
    "        else:\n",
    "            return self.full_rank_coef_matrix()\n",
    "\n",
    "    def one_sample_forw(self, sample):\n",
    "        out = []\n",
    "        for degree in range(self.deg+1):\n",
    "            kroneck = compute_kronecker_product(degree, sample)\n",
    "            out.append(kroneck)\n",
    "        out= torch.concatenate(out)\n",
    "        out= torch.reshape(out, (1,out.shape[0]))\n",
    "        return out\n",
    "\n",
    "    def forward(self, latent):\n",
    "        if latent.shape[0] == 1:\n",
    "            x = self.one_sample_forw(latent)\n",
    "        else:\n",
    "            x = []\n",
    "            for idx in range(latent.shape[0]):\n",
    "                x.append( self.one_sample_forw(latent[idx, :]))\n",
    "            x = torch.cat(x, dim=0)\n",
    "        x1= torch.matmul(x[:, :1+self.lat_dim], self.coff_matrix[:1+self.lat_dim, :])\n",
    "        x2= torch.matmul(x[:, 1+self.lat_dim:], self.coff_matrix[1+self.lat_dim:, :])\n",
    "        norm_factor= 0.5 * torch.max(torch.abs(x2)) / torch.max(torch.abs(x1))\n",
    "        x2 = x2 / norm_factor\n",
    "        x = (x1+x2)\n",
    "        return x\n",
    "\n",
    "class Data:\n",
    "    def __init__(self,\n",
    "                 X_dim,\n",
    "                 latent_dim,\n",
    "                 n_per_env,\n",
    "                 n_env,\n",
    "                 eta,\n",
    "                 device,\n",
    "                 target_intervention = False,\n",
    "                 v_mean = None,\n",
    "                 target_children = False,\n",
    "                 degree = 2,\n",
    "                 n_batch = 10,\n",
    "                 misspecified = False):\n",
    "\n",
    "        self.X_dim = X_dim\n",
    "        self.lat_dim = latent_dim\n",
    "        self.n_per_env = n_per_env\n",
    "        self.n_env = n_env\n",
    "        self.eta = eta\n",
    "        self.device = device\n",
    "        self.target_intervention = target_intervention\n",
    "        self.v_mean = v_mean\n",
    "        self.n_batch = n_batch\n",
    "        self.ncond = n_env # + 1\n",
    "        self.lower_triangular = not target_children\n",
    "        self.degree = degree\n",
    "\n",
    "        self.graph = get_dag((latent_dim + 1), connected = True)\n",
    "        self.B = torch.from_numpy(adjacency(self.graph, l_triangular = self.lower_triangular)).to(torch.float32)\n",
    "        self.C = torch.linalg.inv(torch.eye(latent_dim+1) - self.B).to(torch.float32)\n",
    "        self.b = self.B[-1,:-1].unsqueeze(1)\n",
    "        self.misspecified = misspecified\n",
    "\n",
    "        self.latent_fn()\n",
    "\n",
    "        self.cov_eps = get_covariance(latent_dim + 1, normed = True, zerow = False)\n",
    "\n",
    "        self.means_delta = [np.random.uniform(-3,3,size = latent_dim + 1) for e in range(1,n_env)]\n",
    "        if True:\n",
    "            self.means_delta = [m/np.linalg.norm(m) for m in self.means_delta]\n",
    "        self.covs_delta = [get_covariance(latent_dim + 1, normed = True, zerow = (not target_intervention)) for e in range(1,n_env)] #1 for reference env\n",
    "\n",
    "        self.aa = np.array([self.covs_delta[e] + self.means_delta[e].reshape(-1,1) @ self.means_delta[e].reshape(1,-1) for e in range(self.n_env - 1)]\n",
    "                      ).reshape(self.n_env -1, self.lat_dim+1, self.lat_dim +1)\n",
    "\n",
    "        self.vvt = (self.eta/self.n_env) * np.sum(self.aa, 0)\n",
    "\n",
    "        self.X,self.Y,self.Z,self.E = self.get_train_data()\n",
    "\n",
    "        self.X_test,self.Y_test,self.Z_test = self.get_test_data(v_mean = self.v_mean)\n",
    "\n",
    "        self.loader, self.full_loader = self.get_loader(batch_size=(self.n_per_env*self.n_env)//n_batch)\n",
    "\n",
    "    def latent_fn(self, seed=None):\n",
    "        '''\n",
    "        Nontrained feedfowards to generate X from given Z\n",
    "        '''\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "        with torch.no_grad():\n",
    "            self.lat_fn = Poly_dec(poly_degree=self.degree, x_dim=self.X_dim, lat_dim=self.lat_dim)\n",
    "\n",
    "    def get_train_data(self):\n",
    "        Z = torch.tensor([])\n",
    "        Y = torch.tensor([])\n",
    "        X = torch.tensor([])\n",
    "        E = torch.tensor([])\n",
    "        eye = torch.eye(self.n_env)\n",
    "\n",
    "        for e in range(self.n_env):\n",
    "            eps_e = torch.distributions.multivariate_normal.MultivariateNormal(\n",
    "                torch.zeros(self.lat_dim + 1), torch.from_numpy(self.cov_eps).to(torch.float32)\n",
    "                ).sample((self.n_per_env,))\n",
    "\n",
    "            delta_e = np.zeros((self.n_per_env,self.lat_dim + 1))\n",
    "            if e != 0:\n",
    "                if not self.target_intervention:\n",
    "                    self.means_delta[e-1][-1] = 0.0\n",
    "                delta_e = np.random.multivariate_normal(self.means_delta[e-1], self.covs_delta[e-1], self.n_per_env)\n",
    "            delta_e = torch.from_numpy(delta_e).to(torch.float32)\n",
    "\n",
    "            observations = (eps_e + delta_e) @ self.C.t()\n",
    "            Z_e = observations[:, :-1]\n",
    "            with torch.no_grad():\n",
    "                X_e = self.lat_fn(Z_e)\n",
    "            Y_e = observations[:,-1]\n",
    "            E_e = torch.cat([eye[e,:].unsqueeze(0) for _ in range(self.n_per_env)])\n",
    "            X = torch.cat((X,X_e), dim = 0)\n",
    "            Y = torch.cat((Y,Y_e), dim = 0)\n",
    "            Z = torch.cat((Z,Z_e), dim = 0)\n",
    "            E = torch.cat((E,E_e), dim = 0)\n",
    "        return X,Y.unsqueeze(1),Z,E\n",
    "\n",
    "    def get_test_data(self, v_mean = None):\n",
    "        if v_mean is not None:\n",
    "            v_mu = v_mean\n",
    "        else:\n",
    "            v_mu = (self.eta/self.n_env) * np.sum(self.means_delta, 0)#np.random.normal(size = (self.lat_dim + 1))\n",
    "        if not self.target_intervention:\n",
    "            v_mu[-1] = 0.0\n",
    "            self.vvt[:, -1], self.vvt[-1, :] = 0., 0.\n",
    "\n",
    "        self.v_mean = v_mu #/ np.linalg.norm(v_mu)\n",
    "        if self.misspecified:\n",
    "            eps_plus_v = torch.distributions.chi2.Chi2(torch.tensor([np.linalg.norm(self.v_mean, 1)], dtype=torch.float32)+0.5).sample((self.n_per_env, self.lat_dim+1)).squeeze(-1)\n",
    "            observations = (eps_plus_v) @ self.C.t()\n",
    "        else:\n",
    "            eps = torch.distributions.multivariate_normal.MultivariateNormal(\n",
    "              torch.zeros(self.lat_dim + 1), torch.from_numpy(self.cov_eps).to(torch.float32)\n",
    "              ).sample((self.n_per_env,))\n",
    "\n",
    "            v = np.random.multivariate_normal(self.v_mean,\n",
    "                                              self.vvt - self.v_mean.reshape(-1,1)@self.v_mean.reshape(1,-1),\n",
    "                                              self.n_per_env)\n",
    "            v = torch.from_numpy(v).to(torch.float32)\n",
    "\n",
    "            observations = (eps + v) @ self.C.t()\n",
    "        Z_test = observations[:,:-1]\n",
    "        Y_test = observations[:,-1]\n",
    "        X_test = self.lat_fn(Z_test)\n",
    "        return X_test, Y_test.unsqueeze(1), Z_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy.special import comb\n",
    "from torch import nn\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "class Poly_dec(nn.Module):\n",
    "    \"\"\"\n",
    "    Injective polynomial decoder for the latent space.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, deg, x_dim, lat_dim, debug=False):\n",
    "        super().__init__()\n",
    "        # Properties of data and latent space\n",
    "        self.deg = deg\n",
    "        self.x_dim = x_dim\n",
    "        self.lat_dim = lat_dim\n",
    "        self.debug = debug\n",
    "\n",
    "        # Compute the total number of polynomial terms\n",
    "        self.poly_size = self.num_polynomial_terms_of_degree_p(self.deg)\n",
    "\n",
    "        # Check the implicit dimensionality condition for full column-rankedness\n",
    "        assert self.x_dim >= self.num_polynomial_terms_of_degree_p(self.deg), (\n",
    "            \"The polynomial degree is too high for the latent dimensionality \"\n",
    "            \"to guarantee an injective polynomial decoder.\"\n",
    "        )\n",
    "\n",
    "        # Generate a full-rank coefficient matrix (for injectivity)\n",
    "        self.coef_matrix = self.random_full_column_rank()\n",
    "\n",
    "    def num_polynomial_terms_of_degree_p(self, p):\n",
    "        \"\"\"\n",
    "        Compute the number of polynomial terms for a given degree p.\n",
    "        Uses the combinatorial formula for the number of non-negative\n",
    "        integer solutions to x1 + x2 + ... + xk = p.\n",
    "        \"\"\"\n",
    "        count = 0\n",
    "        for r in range(p + 1):\n",
    "            count += comb(r + self.lat_dim - 1, self.lat_dim - 1)\n",
    "        return int(count)\n",
    "\n",
    "    def compute_total_num_polynomial_terms(self):\n",
    "        \"\"\"\n",
    "        Compute the total number of possible terms for polynomials\n",
    "        up to degree self.deg.\n",
    "        \"\"\"\n",
    "        count = 0\n",
    "        for p in range(self.deg + 1):\n",
    "            count += self.num_polynomial_terms_of_degree_p(p)\n",
    "        return count\n",
    "\n",
    "    def random_full_column_rank(self):\n",
    "        \"\"\"\n",
    "        Generate an n x p matrix (p <= n) with real entries drawn from\n",
    "        a normal distribution. With probability 1, it will be full column rank.\n",
    "        If not, the function regenerates until it is.\n",
    "        \"\"\"\n",
    "        n = self.x_dim\n",
    "        p = self.poly_size\n",
    "\n",
    "        while True:\n",
    "            M = torch.randn(n, p)\n",
    "            rank_M = torch.linalg.matrix_rank(M)\n",
    "            if rank_M == p:\n",
    "                return M\n",
    "\n",
    "    def compute_decoder_polynomial(self, latent):\n",
    "        \"\"\"\n",
    "        Compute the polynomial features of the latent vector up to degree self.deg.\n",
    "        Uses scikit-learn's PolynomialFeatures to include interactions.\n",
    "        \"\"\"\n",
    "        assert latent.shape[0] == self.lat_dim, (\n",
    "            \"The latent dimensionality of the sample is incorrect.\"\n",
    "        )\n",
    "\n",
    "        poly = PolynomialFeatures(degree=self.deg, include_bias=True)  \n",
    "        out = poly.fit_transform(latent.reshape(1, -1)).T  # shape -> (n_features, 1)\n",
    "\n",
    "        return torch.tensor(out, dtype=torch.float32)\n",
    "\n",
    "    def forward(self, latent):\n",
    "        \"\"\"\n",
    "        Forward pass of the polynomial decoder.\n",
    "        1) Compute polynomial terms of latent.\n",
    "        2) Multiply by the learned coefficient matrix.\n",
    "        \"\"\"\n",
    "        latent_poly = self.compute_decoder_polynomial(latent)\n",
    "        X = torch.matmul(self.coef_matrix, latent_poly)\n",
    "\n",
    "        if self.debug:\n",
    "            print(f\"\\nShape of the latent vector: {latent.shape}\")\n",
    "            print(latent)\n",
    "            print(f\"\\nShape of the polynomial terms: {latent_poly.shape}\")\n",
    "            print(latent_poly)\n",
    "            print(f\"\\nShape of coefficients: {self.coef_matrix.shape}\")\n",
    "            print(self.coef_matrix)\n",
    "            print(f\"\\nShape of the output: {X.shape}\")\n",
    "            print(X)\n",
    "\n",
    "        return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of the latent vector: torch.Size([2])\n",
      "tensor([-0.2660,  1.7029])\n",
      "\n",
      "Shape of the polynomial terms: torch.Size([10, 1])\n",
      "tensor([[ 1.0000],\n",
      "        [-0.2660],\n",
      "        [ 1.7029],\n",
      "        [ 0.0708],\n",
      "        [-0.4530],\n",
      "        [ 2.9000],\n",
      "        [-0.0188],\n",
      "        [ 0.1205],\n",
      "        [-0.7715],\n",
      "        [ 4.9386]])\n",
      "\n",
      "Shape of coefficients: torch.Size([10, 10])\n",
      "tensor([[-1.4009e+00, -1.0847e+00,  6.3502e-01, -1.3477e+00, -7.1065e-01,\n",
      "         -4.1534e-01,  9.6490e-01, -1.7012e+00, -1.3121e+00,  1.6491e+00],\n",
      "        [ 1.7056e+00,  9.3666e-01, -5.0263e-01, -5.7158e-01, -1.4997e+00,\n",
      "         -1.0078e-01, -1.0436e-01,  1.3510e+00,  2.9263e-01, -5.0478e-01],\n",
      "        [ 2.0585e-02,  1.0237e+00, -8.7733e-01, -1.4752e+00, -4.0222e-01,\n",
      "         -3.0210e-01, -2.9960e-01,  1.8626e+00,  7.9806e-01, -3.8765e+00],\n",
      "        [ 9.4639e-01,  2.0964e-01, -5.9200e-01,  1.0443e+00, -1.3160e+00,\n",
      "         -2.8928e-01,  7.4246e-01, -1.9900e-01, -1.0329e-01, -4.7303e-02],\n",
      "        [-9.3518e-01,  1.2844e+00, -1.5953e+00,  5.2054e-01,  5.8390e-01,\n",
      "          3.9311e-02,  4.0392e-01,  7.0836e-01, -3.3553e-01,  8.8350e-01],\n",
      "        [-1.3379e-02,  6.0452e-01, -6.3948e-01,  1.2757e-01, -1.5854e-01,\n",
      "          2.5270e+00, -1.2652e+00,  5.2839e-01, -2.4120e-01, -1.1782e+00],\n",
      "        [ 5.7885e-01, -3.4290e-01,  8.4641e-04, -4.5503e-01,  9.4450e-01,\n",
      "          7.3650e-01,  2.8243e-01, -7.6754e-01, -3.9942e-01,  7.3104e-01],\n",
      "        [ 9.0084e-01,  1.9909e-01,  7.4594e-01, -8.3926e-01, -9.4880e-01,\n",
      "          1.8223e+00, -1.2621e+00,  8.4023e-01, -7.2208e-02, -9.3422e-01],\n",
      "        [-3.3130e-01,  1.0051e+00,  1.0792e+00,  4.2415e-01, -1.1212e+00,\n",
      "         -2.3376e-01,  5.7225e-01,  2.4320e-01, -1.3350e+00,  6.4160e-01],\n",
      "        [-5.7227e-02, -7.4137e-01, -7.1758e-01,  1.0924e+00,  8.7295e-01,\n",
      "          7.6689e-01,  7.6154e-01, -1.0878e+00, -5.2667e-01,  6.3900e-01]])\n",
      "\n",
      "Shape of the output: torch.Size([10, 1])\n",
      "tensor([[  7.9243],\n",
      "        [ -1.6068],\n",
      "        [-22.0742],\n",
      "        [ -0.4782],\n",
      "        [  0.5925],\n",
      "        [  0.6012],\n",
      "        [  6.1680],\n",
      "        [  3.3404],\n",
      "        [  5.3163],\n",
      "        [  4.2405]])\n"
     ]
    }
   ],
   "source": [
    "# Test the polynomial decoder\n",
    "poly_degree = 3\n",
    "x_dim = 10\n",
    "lat_dim = 2\n",
    "sample_latent = torch.randn(lat_dim)\n",
    "#sample_latent = torch.tensor([2,3])\n",
    "\n",
    "poly_decoder = Poly_dec(poly_degree, x_dim, lat_dim, debug=True)\n",
    "\n",
    "# Test the forward pass\n",
    "X = poly_decoder.forward(sample_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4733, -0.0166,  0.5124,  0.8485,  1.1833,  1.3318, -0.0339, -0.0971,\n",
       "        -1.3365, -0.9610])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
