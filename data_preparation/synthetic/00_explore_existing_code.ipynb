{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"### Data\"\"\"\n",
    "\n",
    "import copy\n",
    "\n",
    "def compute_total_polynomial_terms(poly_degree, latent_dim):\n",
    "    \"\"\"\n",
    "    Compute the total number of possible terms for polynomials of degree poly_degree.\n",
    "    \"\"\"\n",
    "    count=0\n",
    "    for degree in range(poly_degree+1):\n",
    "        count+= pow(latent_dim, degree)\n",
    "    return count\n",
    "\n",
    "\n",
    "def compute_kronecker_product(degree, latent):\n",
    "    \"\"\"\n",
    "    Compute the kronecker product of the latent vector with itself for a given degree.\n",
    "    \"\"\"\n",
    "    if degree ==0:\n",
    "        out= torch.tensor([1.])\n",
    "    else:\n",
    "        out=copy.deepcopy(latent)\n",
    "        for idx in range(1, degree):\n",
    "            out= torch.kron(out, latent)\n",
    "    #print(out.shape)\n",
    "    return out\n",
    "\n",
    "def compute_decoder_polynomial(poly_degree,latent):\n",
    "    \"\"\"\n",
    "    Compute all the kronecker products of the latent vector with itself up to degree poly_degree.\n",
    "    \"\"\"\n",
    "    out=[]\n",
    "    for degree in range(poly_degree+1):\n",
    "        kroneck = compute_kronecker_product(degree, latent)\n",
    "        out.append(kroneck)\n",
    "    out= torch.concatenate(out)\n",
    "    out= torch.reshape(out, (1,out.shape[0]))\n",
    "    return out\n",
    "\n",
    "def compute_decoder_polynomial_function(poly_degree):\n",
    "    \"\"\"\n",
    "    Compute the function that computes the polynomial terms of a given degree.\n",
    "    \"\"\"\n",
    "    def poly_decoder(latent):\n",
    "        out=[]\n",
    "        for degree in range(poly_degree+1):\n",
    "    #         print('Computing polynomial term of degree ', degree)\n",
    "            kroneck = compute_kronecker_product(degree, latent)\n",
    "            #print(kroneck.shape)\n",
    "            out.append(kroneck)\n",
    "        out= torch.concatenate(out)\n",
    "        out= torch.reshape(out, (1,out.shape[0]))\n",
    "        return out\n",
    "    return poly_decoder\n",
    "\n",
    "class Poly_dec(nn.Module):\n",
    "    def __init__(self, poly_degree, x_dim, lat_dim):\n",
    "        super().__init__()\n",
    "        self.deg = poly_degree\n",
    "        self.x_dim = x_dim\n",
    "        self.lat_dim = lat_dim\n",
    "\n",
    "        self.poly_size = compute_total_polynomial_terms(poly_degree, lat_dim)\n",
    "        #self.coff_matrix = np.random.multivariate_normal(np.zeros(self.poly_size), np.eye(self.poly_size), size=20).T\n",
    "        self.coff_matrix = self.full_rank_coef_matrix()\n",
    "\n",
    "    def full_rank_coef_matrix(self):\n",
    "        \"\"\"\n",
    "        Generate a full rank coefficient matrix for the polynomial decoder.\n",
    "        \"\"\"\n",
    "        M = torch.distributions.multivariate_normal.MultivariateNormal(torch.zeros(self.poly_size), torch.eye(self.poly_size)).sample((self.x_dim,)).t()\n",
    "        if torch.linalg.matrix_rank(M) == self.poly_size:\n",
    "            return M\n",
    "        else:\n",
    "            return self.full_rank_coef_matrix()\n",
    "\n",
    "    def one_sample_forw(self, sample):\n",
    "        out = []\n",
    "        for degree in range(self.deg+1):\n",
    "            kroneck = compute_kronecker_product(degree, sample)\n",
    "            out.append(kroneck)\n",
    "        out= torch.concatenate(out)\n",
    "        out= torch.reshape(out, (1,out.shape[0]))\n",
    "        return out\n",
    "\n",
    "    def forward(self, latent):\n",
    "        if latent.shape[0] == 1:\n",
    "            x = self.one_sample_forw(latent)\n",
    "        else:\n",
    "            x = []\n",
    "            for idx in range(latent.shape[0]):\n",
    "                x.append( self.one_sample_forw(latent[idx, :]))\n",
    "            x = torch.cat(x, dim=0)\n",
    "        x1= torch.matmul(x[:, :1+self.lat_dim], self.coff_matrix[:1+self.lat_dim, :])\n",
    "        x2= torch.matmul(x[:, 1+self.lat_dim:], self.coff_matrix[1+self.lat_dim:, :])\n",
    "        norm_factor= 0.5 * torch.max(torch.abs(x2)) / torch.max(torch.abs(x1))\n",
    "        x2 = x2 / norm_factor\n",
    "        x = (x1+x2)\n",
    "        return x\n",
    "\n",
    "class Data:\n",
    "    def __init__(self,\n",
    "                 X_dim,\n",
    "                 latent_dim,\n",
    "                 n_per_env,\n",
    "                 n_env,\n",
    "                 eta,\n",
    "                 device,\n",
    "                 target_intervention = False,\n",
    "                 v_mean = None,\n",
    "                 target_children = False,\n",
    "                 degree = 2,\n",
    "                 n_batch = 10,\n",
    "                 misspecified = False):\n",
    "\n",
    "        self.X_dim = X_dim\n",
    "        self.lat_dim = latent_dim\n",
    "        self.n_per_env = n_per_env\n",
    "        self.n_env = n_env\n",
    "        self.eta = eta\n",
    "        self.device = device\n",
    "        self.target_intervention = target_intervention\n",
    "        self.v_mean = v_mean\n",
    "        self.n_batch = n_batch\n",
    "        self.ncond = n_env # + 1\n",
    "        self.lower_triangular = not target_children\n",
    "        self.degree = degree\n",
    "\n",
    "        self.graph = get_dag((latent_dim + 1), connected = True)\n",
    "        self.B = torch.from_numpy(adjacency(self.graph, l_triangular = self.lower_triangular)).to(torch.float32)\n",
    "        self.C = torch.linalg.inv(torch.eye(latent_dim+1) - self.B).to(torch.float32)\n",
    "        self.b = self.B[-1,:-1].unsqueeze(1)\n",
    "        self.misspecified = misspecified\n",
    "\n",
    "        self.latent_fn()\n",
    "\n",
    "        self.cov_eps = get_covariance(latent_dim + 1, normed = True, zerow = False)\n",
    "\n",
    "        self.means_delta = [np.random.uniform(-3,3,size = latent_dim + 1) for e in range(1,n_env)]\n",
    "        if True:\n",
    "            self.means_delta = [m/np.linalg.norm(m) for m in self.means_delta]\n",
    "        self.covs_delta = [get_covariance(latent_dim + 1, normed = True, zerow = (not target_intervention)) for e in range(1,n_env)] #1 for reference env\n",
    "\n",
    "        self.aa = np.array([self.covs_delta[e] + self.means_delta[e].reshape(-1,1) @ self.means_delta[e].reshape(1,-1) for e in range(self.n_env - 1)]\n",
    "                      ).reshape(self.n_env -1, self.lat_dim+1, self.lat_dim +1)\n",
    "\n",
    "        self.vvt = (self.eta/self.n_env) * np.sum(self.aa, 0)\n",
    "\n",
    "        self.X,self.Y,self.Z,self.E = self.get_train_data()\n",
    "\n",
    "        self.X_test,self.Y_test,self.Z_test = self.get_test_data(v_mean = self.v_mean)\n",
    "\n",
    "        self.loader, self.full_loader = self.get_loader(batch_size=(self.n_per_env*self.n_env)//n_batch)\n",
    "\n",
    "    def latent_fn(self, seed=None):\n",
    "        '''\n",
    "        Nontrained feedfowards to generate X from given Z\n",
    "        '''\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "        with torch.no_grad():\n",
    "            self.lat_fn = Poly_dec(poly_degree=self.degree, x_dim=self.X_dim, lat_dim=self.lat_dim)\n",
    "\n",
    "    def get_train_data(self):\n",
    "        Z = torch.tensor([])\n",
    "        Y = torch.tensor([])\n",
    "        X = torch.tensor([])\n",
    "        E = torch.tensor([])\n",
    "        eye = torch.eye(self.n_env)\n",
    "\n",
    "        for e in range(self.n_env):\n",
    "            eps_e = torch.distributions.multivariate_normal.MultivariateNormal(\n",
    "                torch.zeros(self.lat_dim + 1), torch.from_numpy(self.cov_eps).to(torch.float32)\n",
    "                ).sample((self.n_per_env,))\n",
    "\n",
    "            delta_e = np.zeros((self.n_per_env,self.lat_dim + 1))\n",
    "            if e != 0:\n",
    "                if not self.target_intervention:\n",
    "                    self.means_delta[e-1][-1] = 0.0\n",
    "                delta_e = np.random.multivariate_normal(self.means_delta[e-1], self.covs_delta[e-1], self.n_per_env)\n",
    "            delta_e = torch.from_numpy(delta_e).to(torch.float32)\n",
    "\n",
    "            observations = (eps_e + delta_e) @ self.C.t()\n",
    "            Z_e = observations[:, :-1]\n",
    "            with torch.no_grad():\n",
    "                X_e = self.lat_fn(Z_e)\n",
    "            Y_e = observations[:,-1]\n",
    "            E_e = torch.cat([eye[e,:].unsqueeze(0) for _ in range(self.n_per_env)])\n",
    "            X = torch.cat((X,X_e), dim = 0)\n",
    "            Y = torch.cat((Y,Y_e), dim = 0)\n",
    "            Z = torch.cat((Z,Z_e), dim = 0)\n",
    "            E = torch.cat((E,E_e), dim = 0)\n",
    "        return X,Y.unsqueeze(1),Z,E\n",
    "\n",
    "    def get_test_data(self, v_mean = None):\n",
    "        if v_mean is not None:\n",
    "            v_mu = v_mean\n",
    "        else:\n",
    "            v_mu = (self.eta/self.n_env) * np.sum(self.means_delta, 0)#np.random.normal(size = (self.lat_dim + 1))\n",
    "        if not self.target_intervention:\n",
    "            v_mu[-1] = 0.0\n",
    "            self.vvt[:, -1], self.vvt[-1, :] = 0., 0.\n",
    "\n",
    "        self.v_mean = v_mu #/ np.linalg.norm(v_mu)\n",
    "        if self.misspecified:\n",
    "            eps_plus_v = torch.distributions.chi2.Chi2(torch.tensor([np.linalg.norm(self.v_mean, 1)], dtype=torch.float32)+0.5).sample((self.n_per_env, self.lat_dim+1)).squeeze(-1)\n",
    "            observations = (eps_plus_v) @ self.C.t()\n",
    "        else:\n",
    "            eps = torch.distributions.multivariate_normal.MultivariateNormal(\n",
    "              torch.zeros(self.lat_dim + 1), torch.from_numpy(self.cov_eps).to(torch.float32)\n",
    "              ).sample((self.n_per_env,))\n",
    "\n",
    "            v = np.random.multivariate_normal(self.v_mean,\n",
    "                                              self.vvt - self.v_mean.reshape(-1,1)@self.v_mean.reshape(1,-1),\n",
    "                                              self.n_per_env)\n",
    "            v = torch.from_numpy(v).to(torch.float32)\n",
    "\n",
    "            observations = (eps + v) @ self.C.t()\n",
    "        Z_test = observations[:,:-1]\n",
    "        Y_test = observations[:,-1]\n",
    "        X_test = self.lat_fn(Z_test)\n",
    "        return X_test, Y_test.unsqueeze(1), Z_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import comb\n",
    "from torch import nn\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "class Poly_dec(nn.Module):\n",
    "    \"\"\" \n",
    "    Injective polynomial decoder for the latent space.\n",
    "    \"\"\"\n",
    "    def __init__(self, deg, x_dim, lat_dim, debug=False):\n",
    "        super().__init__()\n",
    "        # Properties of data and latent space\n",
    "        self.deg = deg\n",
    "        self.x_dim = x_dim\n",
    "        self.lat_dim = lat_dim\n",
    "        self.debug = debug\n",
    "\n",
    "        # Compute the total number of polynomial terms\n",
    "        self.poly_size = self.num_polynomial_terms_of_degree_p(self.deg) #self.compute_num_polynomial_terms()\n",
    "\n",
    "        # Check the implicit dimensionality condition for full column-rankedness\n",
    "        assert self.x_dim >= self.num_polynomial_terms_of_degree_p(self.deg), \"The polynomial degree is too high for the latent dimensionality to guarantee an injective polynomial decoder.\"\n",
    "\n",
    "        # Generate a full rank coefficient matrix - full rank for injectivity\n",
    "        self.coef_matrix = self.random_full_column_rank()\n",
    "        \n",
    "    \n",
    "    def num_polynomial_terms_of_degree_p(self, p):\n",
    "        \"\"\"\n",
    "        Compute the number of polynomial terms for a given degree.\n",
    "        \"\"\"\n",
    "        count = 0 \n",
    "\n",
    "        # Using the combinatorial formula for the number of non-negative integer solutions to the equation x1 + x2 + ... + xk = p\n",
    "        for r in range(p+1):\n",
    "            count += comb(r + self.lat_dim - 1, self.lat_dim - 1)\n",
    "\n",
    "        return int(count)\n",
    "\n",
    "    def compute_total_num_polynomial_terms(self):\n",
    "        \"\"\"\n",
    "        Compute the total number of possible terms for polynomials of degree deg.\n",
    "        \"\"\"\n",
    "        count = 0\n",
    "        for p in range(self.deg + 1):\n",
    "            count += self.num_polynomial_terms_of_degree_p(p)\n",
    "        return count\n",
    "        \n",
    "    def random_full_column_rank(self):\n",
    "        \"\"\"\n",
    "        Generate an n x p matrix (p <= n) with real entries drawn from a \n",
    "        normal distribution. With probability 1, it will be full column rank.\n",
    "        If not, the function regenerates until it is.\n",
    "        \"\"\"\n",
    "        n = self.x_dim\n",
    "        p = self.poly_size\n",
    "        # print(f\"Generating a full column rank matrix of size {n} x {p}...\")\n",
    "\n",
    "        while True:\n",
    "            # Generate a random matrix with high probability of being full column rank\n",
    "            M = torch.randn(n, p)\n",
    "\n",
    "            # Check if the matrix is full column rank\n",
    "            rank_M = torch.linalg.matrix_rank(M)\n",
    "            if rank_M == p:\n",
    "                return M\n",
    "\n",
    "    def compute_decoder_polynomial(self, latent):\n",
    "        \"\"\"\n",
    "        Compute all the kronecker products with distinct entries of the latent vector \n",
    "        with itself up to degree poly_degree. \n",
    "        \"\"\"\n",
    "        assert latent.shape[0] == self.lat_dim, \"The latent dimensionality of the sample is incorrect.\"\n",
    "\n",
    "        out = []\n",
    "     \n",
    "        poly = PolynomialFeatures(degree=self.deg, include_bias=True)  # exclude the constant term\n",
    "        out = poly.fit_transform(latent.reshape(1, -1)).T  # shape = (1, number_of_features)\n",
    "    \n",
    "        return torch.tensor(out, dtype=torch.float32)\n",
    "    \n",
    "    def forward(self, latent):\n",
    "        \"\"\"\n",
    "        Forward pass of the polynomial decoder.\n",
    "        \"\"\"\n",
    "        latent_poly = self.compute_decoder_polynomial(latent)\n",
    "\n",
    "        \n",
    "        # Apply coefficients to the polynomial terms\n",
    "        X = torch.matmul(self.coef_matrix, latent_poly)\n",
    "\n",
    "        if self.debug:\n",
    "            print(f\"\\nShape of the latent vector: {latent.shape}\")\n",
    "            print(latent)\n",
    "            print(f\"\\nShape of the polynomial terms: {latent_poly.shape}\")\n",
    "            print(latent_poly)\n",
    "            print(f\"\\nShape of coefficients: {self.coef_matrix.shape}\")\n",
    "            print(self.coef_matrix)\n",
    "            print(f\"\\nShape of the output: {X.shape}\")\n",
    "            print(X)\n",
    "            \n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The polynomial degree is too high for the latent dimensionality.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m sample_latent \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(lat_dim)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#sample_latent = torch.tensor([2,3])\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m poly_decoder \u001b[38;5;241m=\u001b[39m \u001b[43mPoly_dec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoly_degree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlat_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Test the forward pass\u001b[39;00m\n\u001b[1;32m     11\u001b[0m X \u001b[38;5;241m=\u001b[39m poly_decoder\u001b[38;5;241m.\u001b[39mforward(sample_latent)\n",
      "Cell \u001b[0;32mIn[68], line 21\u001b[0m, in \u001b[0;36mPoly_dec.__init__\u001b[0;34m(self, deg, x_dim, lat_dim, debug)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoly_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_polynomial_terms_of_degree_p(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeg) \u001b[38;5;66;03m#self.compute_num_polynomial_terms()\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Check the implicit dimensionality condition for full column-rankedness\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_dim \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_polynomial_terms_of_degree_p(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeg), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe polynomial degree is too high for the latent dimensionality.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Generate a full rank coefficient matrix - full rank for injectivity\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_full_column_rank()\n",
      "\u001b[0;31mAssertionError\u001b[0m: The polynomial degree is too high for the latent dimensionality."
     ]
    }
   ],
   "source": [
    "# Test the polynomial decoder\n",
    "poly_degree = 4\n",
    "x_dim = 6\n",
    "lat_dim = 2\n",
    "sample_latent = torch.randn(lat_dim)\n",
    "#sample_latent = torch.tensor([2,3])\n",
    "\n",
    "poly_decoder = Poly_dec(poly_degree, x_dim, lat_dim, debug=True)\n",
    "\n",
    "# Test the forward pass\n",
    "X = poly_decoder.forward(sample_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4733, -0.0166,  0.5124,  0.8485,  1.1833,  1.3318, -0.0339, -0.0971,\n",
       "        -1.3365, -0.9610])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
